---
author: Saulo Gil
categories:
- Theme Features
- R
- package
date: "2024-01-06"
draft: false
excerpt: This project used XGboost to predict dyspnea in COVID-19 survivors 6-9 months after hospital discharge. 
  websites.
featured: true
layout: single-sidebar
subtitle: "In progress"
tags:
- hugo-site
title: Using XGboost to Predict Dyspnea in COVID-19 Survivors
---
# Introduction
xxxxxxxxxxx

## Packages
```{r message=FALSE, warning=FALSE}
# Pacotes
library(tidyverse)
library(tidymodels)
library(GGally)
library(DataExplorer)
library(vip)
library(doParallel)
```

## Reading dataset
```{r}
# Lendo a base
dispneia <- read.csv2('df_ajustada.csv')
```

## Spliting dataset - Train/Test
```{r}
set.seed(32)

dispneia_initial_split <- initial_split(dispneia,
                                        prop = 0.8,
                                        strata = "dispneia")
dispneia_initial_split

# train and test datasets
dispneia_train <- training(dispneia_initial_split)

dispneia_test <- testing(dispneia_initial_split)
```

## Exploratory analysis 
### Overview
```{r}
skimr::skim(dispneia_train)
```

### Missing values

```{r, fig.height=12, fig.width=14, message=FALSE, warning=FALSE}
plot_missing(dispneia_train)
```

## Checking the correlation and distribution of numerical features

```{r, out.width="100%"}
dispneia_train |>  
  select(where(is.numeric)) |>  
  cor(use = "pairwise.complete.obs") |> 
  corrplot::corrplot(method = "number")
```
```{r, fig.height=14, fig.width=14, message=FALSE, warning=FALSE}
dispneia_train |>  
  select(where(is.numeric), dispneia) |> 
  ggpairs(aes(colour = dispneia))
```

```{r, fig.height=14, fig.width=16}
dispneia_train |>  
  select(c(where(is.numeric), dispneia)) |> 
  pivot_longer(-dispneia, 
               names_to = "Feature",
               values_to = "Value") |> 
  ggplot(aes(y = dispneia, 
             x = Value, 
             fill = dispneia)) +
  geom_boxplot() +
  facet_wrap(~Feature,
             scales = "free_x") +
  ggtitle("Dyspnea vs. Numeric Features")
```

```{r, fig.height=14, fig.width=16}
dispneia_train |>  
  select(c(where(is.numeric), dispneia)) |> 
  pivot_longer(-dispneia, 
               names_to = "Feature",
               values_to = "Value") |> 
  ggplot(aes(x = Value, 
             colour = dispneia)) +
  stat_ecdf() +
  facet_wrap(~Feature, scales = "free_x") +
  labs(title = "Dyspnea vs. Numeric Features",
       subtitle = "Cumulative Distribution")
```

```{r, fig.height=14, fig.width=16}
grafico_de_barras_das_vars_continuas <- 
  function(dados) {
    dados |>  
    select(c(where(is.numeric), dispneia)) |> 
    pivot_longer(-dispneia,
                 names_to = "Feature",
                 values_to = "Value") |> 
    dplyr::group_by(Feature) |> 
    dplyr::mutate(
      Value = factor(dplyr::ntile(Value, 10),
                     levels = 1:10)
    ) |> 
    dplyr::count(dispneia, 
                 Feature,
                 Value) |> 
    ggplot(aes(y = (Value),
               x = n,
               fill = dispneia)) +
    geom_col(position = "fill") +
    geom_label(aes(label = n),
               position = position_fill(vjust = 0.5)) +
    facet_wrap(~Feature,
               scales = "free_y",
               ncol = 3) +
    ggtitle("Dyspnea vs. Numeric Features")
}

grafico_de_barras_das_vars_continuas(dispneia_train)
```

## Checking the distribution of categorical features
```{r, fig.height=8}
contagens <- 
  dispneia_train |>  
  select(c(where(is.character),
           dispneia)) |> 
  pivot_longer(-dispneia,
               names_to = "Feature",
               values_to = "Value") |> 
  count(dispneia,
        Feature,
        Value)

# tabela
contagens |> 
  pivot_wider(names_from = dispneia,
              values_from = n) |> 
  DT::datatable()
```

```{r, fig.height=16, fig.width=16}
contagens |> 
  ggplot(aes(y = Value,
             x = n,
             fill = dispneia)) +
  geom_col(position = "fill") +
  geom_label(aes(label = n),
             position = position_fill(vjust = 0.5)) +
  facet_wrap(~Feature,
             scales = "free_y",
             ncol = 3) +
  ggtitle("Dyspnea vs. Categorical Features")
```

## Preprocessing and Feature Engineering Steps using Recipes package
# Recipe
```{r}
dispneia_recipe <-
  recipe(formula = dispneia ~ ., dispneia_train) |>
  step_normalize(all_numeric_predictors()) |> # normalize all continuous features
  step_corr(all_numeric_predictors(), threshold = .8) |>  # multicollinearity <- r > 0.8
  step_impute_median(all_numeric_predictors()) |>  # Processing missing continuous data - imputing median 
  step_impute_mode(all_nominal_predictors()) |> # Processing missing categorical data - imputing more frequent class
  step_zv(all_predictors()) |>  # removing variables that contain only zero (zero variance filter)
  step_dummy(all_nominal_predictors()) # dummy - nominal predictors


# Checking the result of the recipe (and whether the steps worked!)
dispneia_recipe |> 
    prep() |> 
    bake(new_data = dispneia_train) |> 
    glimpse()
```

## Definition of cross-validation
```{r}
dispneia_resamples <- 
  vfold_cv(dispneia_train,
           v = 5, 
           strata = dispneia) # 3 folds due to limited dataset

dispneia_resamples$splits
```

## Modeling
## XGBoost
### Step 1 - Number of trees (trees) and learn rate (learn_rate)
### Model
```{r}
# Modelo
dispneia_xgb_model <-
  boost_tree(
  min_n = 15,
  mtry = 0.8,
  trees = tune(),
  tree_depth = 5,
  learn_rate = tune(),
  loss_reduction = 0,
  sample_size = 0.8
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_model) |> 
  add_recipe(dispneia_recipe)

```

### Tuning
```{r}
# Tune
dispneia_grid_xgb <-
  expand.grid(
    trees = seq(25, 800, 25),
    learn_rate = seq(0.01, 0.4,0.1)
)

dispneia_grid_xgb |> 
  DT::datatable()
```

```{r warning=FALSE}
doParallel::registerDoParallel(4)

dispneia_xgb_tune_grid <-
  tune_grid(
  dispneia_xgb_wf,
  resamples = dispneia_resamples,
  grid = dispneia_grid_xgb,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
  verbose = TRUE
  )
```

```{r, out.width="100%"}
autoplot(dispneia_xgb_tune_grid) +
  theme_bw() +
  theme(legend.position = "top")

collect_metrics(dispneia_xgb_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  filter(mean == max(mean))

dispneia_xgb_tune_grid |> 
  select_best(metric = "roc_auc")

show_best(dispneia_xgb_tune_grid, n = 6)

dispneia_best_step1 <-
  dispneia_xgb_tune_grid |>
  select_best(metric = "roc_auc")

# best hiperparameters
glue::glue("The best number of trees was {dispneia_best_step1$trees} and learn_rate was {dispneia_best_step1$learn_rate}.")
```

### Step 2 -  Minimal node size (min_n) and depth tree (tree_depth)
### Model
```{r}
# Modelo
dispneia_xgb_model <-
  boost_tree(
  min_n = tune(),
  mtry = 0.8,
  trees = 25,
  tree_depth = tune(),
  learn_rate = 0.31,
  loss_reduction = 0,
  sample_size = 0.8
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_model) |> 
  add_recipe(dispneia_recipe)

```

### Tuning
```{r}
# Tune - min_n and tree_depth
dispneia_grid_xgb <-
  expand.grid(
    min_n = seq(1,16,2),
    tree_depth = seq(1,17,4)
)

dispneia_grid_xgb |> 
  DT::datatable()
```
```{r warning=FALSE}
doParallel::registerDoParallel(4)

dispneia_xgb_tune_grid <-
  tune_grid(
  dispneia_xgb_wf,
  resamples = dispneia_resamples,
  grid = dispneia_grid_xgb,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
  verbose = TRUE
  )
```

```{r, out.width="100%"}
autoplot(dispneia_xgb_tune_grid) +
  theme_bw() +
  theme(legend.position = "top")

collect_metrics(dispneia_xgb_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  filter(mean == max(mean))

dispneia_xgb_tune_grid |> 
  select_best(metric = "roc_auc")


show_best(dispneia_xgb_tune_grid, n = 6)

dispneia_best_step2 <-
  dispneia_xgb_tune_grid |>
  select_best(metric = "roc_auc")

# best hiperparameters
glue::glue("The best minimal node size was {dispneia_best_step2$min_n} and depth tree was {dispneia_best_step2$tree_depth}.")
```
### Step 3 -  Number for the reduction in the loss function required to split further (loss_reduction)
### Model
```{r}
# Modelo
dispneia_xgb_model <-
  boost_tree(
  min_n = 11,
  mtry = 0.8,
  trees = 25,
  tree_depth = 1,
  learn_rate = 0.31,
  loss_reduction = tune(),
  sample_size = 0.8
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_model) |> 
  add_recipe(dispneia_recipe)

```

### Tuning
```{r}
# Tune - min_n and tree_depth
dispneia_grid_xgb <-
  expand.grid(
    loss_reduction = seq(1, 4, 0.04)
)

dispneia_grid_xgb |> 
  DT::datatable()
```
```{r warning=FALSE}
doParallel::registerDoParallel(4)

dispneia_xgb_tune_grid <-
  tune_grid(
  dispneia_xgb_wf,
  resamples = dispneia_resamples,
  grid = dispneia_grid_xgb,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
  verbose = TRUE
  )
```

```{r, out.width="100%"}
autoplot(dispneia_xgb_tune_grid) +
  theme_bw() 

collect_metrics(dispneia_xgb_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  filter(mean == max(mean))

dispneia_xgb_tune_grid |> 
  select_best(metric = "roc_auc")


show_best(dispneia_xgb_tune_grid, n = 6)

dispneia_best_step3 <-
  dispneia_xgb_tune_grid |>
  select_best(metric = "roc_auc")

# best hiperparameters
glue::glue("The best loss_reduction was {dispneia_best_step3$loss_reduction}.")
```

### Step 4 -  Number (or proportion) of predictors that will be randomly sampled (mtry) and number (or proportion) of data that is exposed to the fitting routine (sample_size)
### Model
```{r}
# Modelo
dispneia_xgb_model <-
  boost_tree(
  min_n = 11,
  mtry = tune(),
  trees = 25,
  tree_depth = 1,
  learn_rate = 0.31,
  loss_reduction = 1.36,
  sample_size = tune()
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_model) |> 
  add_recipe(dispneia_recipe)

```

### Tuning
```{r}
# Tune - min_n and tree_depth
dispneia_grid_xgb <-
  expand.grid(
    mtry = seq(0.2, 1.0, 0.2),
    sample_size = seq(0.2, 1.0,by = 0.2)
)

dispneia_grid_xgb |> 
  DT::datatable()
```

```{r warning=FALSE}
doParallel::registerDoParallel(4)

dispneia_xgb_tune_grid <-
  tune_grid(
  dispneia_xgb_wf,
  resamples = dispneia_resamples,
  grid = dispneia_grid_xgb,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
  verbose = TRUE
  )
```

```{r, out.width="100%"}
autoplot(dispneia_xgb_tune_grid) +
  theme_bw() +
  theme(legend.position = "top")

collect_metrics(dispneia_xgb_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  filter(mean == max(mean))

dispneia_xgb_tune_grid |> 
  select_best(metric = "roc_auc")

show_best(dispneia_xgb_tune_grid, n = 6)

dispneia_best_step4 <-
  dispneia_xgb_tune_grid |>
  select_best(metric = "roc_auc")

# best hiperparameters
glue::glue("The best number (or proportion) of predictors that will be randomly sampled (mtry) was {dispneia_best_step4$mtry} and the number (or proportion) of data that is exposed to the fitting routine (sample size) was {dispneia_best_step4$sample_size}.")
```

### Step 5 -  Training again learing rate and trees with lower values
### Model
```{r}
# Modelo
dispneia_xgb_model <-
  boost_tree(
  min_n = 11,
  mtry = 0.8,
  trees = tune(),
  tree_depth = 1,
  learn_rate = tune(),
  loss_reduction = 1.36,
  sample_size = 0.8
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_model) |> 
  add_recipe(dispneia_recipe)
```

### Tuning
```{r}
# Tune - min_n and tree_depth
dispneia_grid_xgb <-
  expand.grid(
    trees = seq(15, 700, 25),
    learn_rate = seq(0.02, 0.1, 0.02)
)

dispneia_grid_xgb |> 
  DT::datatable()
```

```{r warning=FALSE}
doParallel::registerDoParallel(4)

dispneia_xgb_tune_grid <-
  tune_grid(
  dispneia_xgb_wf,
  resamples = dispneia_resamples,
  grid = dispneia_grid_xgb,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
  verbose = TRUE
  )
```

```{r, out.width="100%"}
autoplot(dispneia_xgb_tune_grid) +
  theme_bw() +
  theme(legend.position = "top")

collect_metrics(dispneia_xgb_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  filter(mean == max(mean))

dispneia_xgb_tune_grid |> 
  select_best(metric = "roc_auc")

show_best(dispneia_xgb_tune_grid, n = 6)

dispneia_best_step5 <-
  dispneia_xgb_tune_grid |>
  select_best(metric = "roc_auc")

# best hiperparameters
glue::glue("Following tunning again, the best number of trees was {dispneia_best_step5$trees} and learn_rate was {dispneia_best_step5$learn_rate}.")
```

### Final model performance
### Model
```{r}
# Model
dispneia_xgb_final_model <-
  boost_tree(
  min_n = 11,
  mtry = 0.8,
  trees = 565,
  tree_depth = 1,
  learn_rate = 0.02,
  loss_reduction = 1.36,
  sample_size = 0.8
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_final_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_final_model) |> 
  add_recipe(dispneia_recipe)
```

### Tuning
```{r}
# Tune - min_n and tree_depth
dispneia_best_params_xgb <-
  expand.grid(
    min_n = 11,
    mtry = 0.8,
    trees = 565,
    tree_depth = 1,
    learn_rate = 0.02,
    loss_reduction = 1.36,
    sample_size = 0.8
)

dispneia_best_params_xgb |> 
  DT::datatable()
```

```{r warning=FALSE}
# Desempenho do modelo finaL ----------------------------------------------
dispneia_xgb_final_wf <- 
  dispneia_xgb_final_wf |>
  finalize_workflow(dispneia_best_params_xgb)

dispneia_xgb_last_fit <-
  last_fit(dispneia_xgb_final_wf, dispneia_initial_split)

collect_predictions(dispneia_xgb_last_fit) |> 
  mutate(modelo = "XGBoost") |> 
  group_by(modelo) |> 
  roc_curve(dispneia, .pred_No) |> 
  autoplot() +
  theme_bw() +
  theme(legend.position = "none")
```

```{r}
dispneia_xgb_last_fit_model <- dispneia_xgb_last_fit$.workflow[[1]]$fit$fit

dispneia_xgb_vip <- 
  extract_fit_engine(dispneia_xgb_last_fit)

vip::vip(dispneia_xgb_vip)+
  theme_bw()

```

# Conclusions
xxxxxxxxxxx
