---
author: Saulo Gil
categories:
- Theme Features
- R
- package
date: "2024-01-06"
draft: false
excerpt: This project used XGboost to predict dyspnea in COVID-19 survivors 6-9 months after hospital discharge. 
  websites.
featured: true
layout: single-sidebar
subtitle: "In progress"
tags:
- hugo-site
title: Using XGboost to Predict Dyspnea in COVID-19 Survivors
---
# Introduction
Dyspnea is a medical term that refers to the sensation of shortness of breath or difficulty breathing. It is a common symptom of heart and lung diseases, such as asthma, pneumonia, and chronic obstructive pulmonary disease (COPD).

In the context of COVID-19, dyspnea is one of the most common symptoms experienced by infected individuals, but it remains even after acute COVID-19 is solved. According to a study published in BMC Pulmonary Medicine, 45% of COVID-19 pneumonia survivors admitted to the hospital experience dyspnea even after 1 year from hospital discharge. In this sense, identifying patients at risk of presenting dyspnea is crucial for early management.

XGBoost (eXtreme Gradient Boosting) is a very popular ML algorithm. Its popularity is due to the fact that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. Therefore, the implementation of ML algorithm XGBoost in clinical setting may be useful for predicting COVID-19 survivors at the risk of dyspnea following discharge.

Herein, we utilized the XGBoost to predict dyspnea in COVID-19 survivors 6-9 months after hospital discharge 

## Packages
```{r message=FALSE, warning=FALSE}
# Pacotes
library(tidyverse)
library(tidymodels)
library(GGally)
library(DataExplorer)
library(vip)
library(doParallel)
```

## Reading dataset
The dataset consist in a real dataset collected in the largest tertiary hospital of Latin America (Clinical Hospital, School of Medicine of the University of Sao Paulo). 

```{r}
# Lendo a base
dispneia <- read.csv2('df_ajustada.csv')
```

## Spliting dataset - Train/Test
Since our dataset is size-limited, I opted to separate 80% of the sample size for training.

```{r}
set.seed(32)

dispneia_initial_split <- initial_split(dispneia,
                                        prop = 0.8,
                                        strata = "dispneia")
dispneia_initial_split

# train and test datasets
dispneia_train <- training(dispneia_initial_split)

dispneia_test <- testing(dispneia_initial_split)
```

## Exploratory analysis 
### Overview
```{r}
skimr::skim(dispneia_train)
```

### Missing values

```{r, fig.height=12, fig.width=14, message=FALSE, warning=FALSE}
plot_missing(dispneia_train)
```

## Checking the correlation and distribution of numerical features

```{r, out.width="100%"}
dispneia_train |>  
  select(where(is.numeric)) |>  
  cor(use = "pairwise.complete.obs") |> 
  corrplot::corrplot(method = "number")
```
```{r, fig.height=14, fig.width=14, message=FALSE, warning=FALSE}
dispneia_train |>  
  select(where(is.numeric), dispneia) |> 
  ggpairs(aes(colour = dispneia))
```

```{r, fig.height=14, fig.width=16}
dispneia_train |>  
  select(c(where(is.numeric), dispneia)) |> 
  pivot_longer(-dispneia, 
               names_to = "Feature",
               values_to = "Value") |> 
  ggplot(aes(y = dispneia, 
             x = Value, 
             fill = dispneia)) +
  geom_boxplot() +
  facet_wrap(~Feature,
             scales = "free_x") +
  ggtitle("Dyspnea vs. Numeric Features")
```

```{r, fig.height=14, fig.width=16}
dispneia_train |>  
  select(c(where(is.numeric), dispneia)) |> 
  pivot_longer(-dispneia, 
               names_to = "Feature",
               values_to = "Value") |> 
  ggplot(aes(x = Value, 
             colour = dispneia)) +
  stat_ecdf() +
  facet_wrap(~Feature, scales = "free_x") +
  labs(title = "Dyspnea vs. Numeric Features",
       subtitle = "Cumulative Distribution")
```

```{r, fig.height=14, fig.width=16}
grafico_de_barras_das_vars_continuas <- 
  function(dados) {
    dados |>  
    select(c(where(is.numeric), dispneia)) |> 
    pivot_longer(-dispneia,
                 names_to = "Feature",
                 values_to = "Value") |> 
    dplyr::group_by(Feature) |> 
    dplyr::mutate(
      Value = factor(dplyr::ntile(Value, 10),
                     levels = 1:10)
    ) |> 
    dplyr::count(dispneia, 
                 Feature,
                 Value) |> 
    ggplot(aes(y = (Value),
               x = n,
               fill = dispneia)) +
    geom_col(position = "fill") +
    geom_label(aes(label = n),
               position = position_fill(vjust = 0.5)) +
    facet_wrap(~Feature,
               scales = "free_y",
               ncol = 3) +
    ggtitle("Dyspnea vs. Numeric Features")
}

grafico_de_barras_das_vars_continuas(dispneia_train)
```

## Checking the distribution of categorical features
```{r, fig.height=8}
contagens <- 
  dispneia_train |>  
  select(c(where(is.character),
           dispneia)) |> 
  pivot_longer(-dispneia,
               names_to = "Feature",
               values_to = "Value") |> 
  count(dispneia,
        Feature,
        Value)

# tabela
contagens |> 
  pivot_wider(names_from = dispneia,
              values_from = n) |> 
  DT::datatable()
```

```{r, fig.height=16, fig.width=16}
contagens |> 
  ggplot(aes(y = Value,
             x = n,
             fill = dispneia)) +
  geom_col(position = "fill") +
  geom_label(aes(label = n),
             position = position_fill(vjust = 0.5)) +
  facet_wrap(~Feature,
             scales = "free_y",
             ncol = 3) +
  ggtitle("Dyspnea vs. Categorical Features")
```

## Preprocessing and Feature Engineering Steps using Recipes package
# Recipe

For numeric features, the values were normalized, multicollineariry higher than 0.8 were removed and missing values were imputed with median.

For nominal features, missing values were imputed with more frequent value, and all features were transformed in a dummy feature.

Features containing only zero were removed.

The processed dataset can be seen below.

```{r}
dispneia_recipe <-
  recipe(formula = dispneia ~ ., dispneia_train) |>
  step_normalize(all_numeric_predictors()) |> # normalize all continuous features
  step_corr(all_numeric_predictors(), threshold = .8) |>  # multicollinearity <- r > 0.8
  step_impute_median(all_numeric_predictors()) |>  # Processing missing continuous data - imputing median 
  step_impute_mode(all_nominal_predictors()) |> # Processing missing categorical data - imputing more frequent class
  step_zv(all_predictors()) |>  # removing variables that contain only zero (zero variance filter)
  step_dummy(all_nominal_predictors()) # dummy - nominal predictors

# Checking the result of the recipe (and whether the steps worked!)
dispneia_recipe |> 
    prep() |> 
    bake(new_data = dispneia_train) |> 
    glimpse()
```

## Definition of cross-validation
```{r}
dispneia_resamples <- 
  vfold_cv(dispneia_train,
           v = 5, 
           strata = dispneia) # 5 folds

dispneia_resamples$splits
```

## Modeling
## XGBoost
The hyperameters tuned were:

 - boost_tree;
 - min_n: An integer for the minimum number of data points in a node that is required for the node to be split further;
 - mtry: A number for the number (or proportion) of predictors that will be randomly sampled at each split when creating the tree models (specific engines only);
 - trees: An integer for the number of trees contained in the ensemble;
 - tree_depth: An integer for the maximum depth of the tree (i.e. number of splits) (specific engines only);
 - learn_rate: A number for the rate at which the boosting algorithm adapts from iteration-to-iteration (specific engines only). This is sometimes referred to as the shrinkage parameter;
 - loss_reduction: A number for the reduction in the loss function required to split further (specific engines only);
 - sample_size: A number for the number (or proportion) of data that is exposed to the fitting routine. For xgboost, the sampling is done at each iteration while C5.0 samples once during training.

The tuning process were done in 5 steps.

### Step 1 - Number of trees (trees) and learn rate (learn_rate)
### Model
```{r}
# Modelo
dispneia_xgb_model <-
  boost_tree(
  min_n = 15,
  mtry = 0.8,
  trees = tune(),
  tree_depth = 5,
  learn_rate = tune(),
  loss_reduction = 0,
  sample_size = 0.8
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_model) |> 
  add_recipe(dispneia_recipe)

```

### Tuning
```{r}
# Tune
dispneia_grid_xgb <-
  expand.grid(
    trees = seq(25, 800, 25),
    learn_rate = seq(0.01, 0.4,0.1)
)

dispneia_grid_xgb |> 
  DT::datatable()
```

```{r warning=FALSE}
doParallel::registerDoParallel(4)

dispneia_xgb_tune_grid <-
  tune_grid(
  dispneia_xgb_wf,
  resamples = dispneia_resamples,
  grid = dispneia_grid_xgb,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
  verbose = TRUE
  )
```

```{r warning=FALSE, out.width="100%"}
autoplot(dispneia_xgb_tune_grid) +
  theme_bw() +
  theme(legend.position = "top")

collect_metrics(dispneia_xgb_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  filter(mean == max(mean))

dispneia_xgb_tune_grid |> 
  select_best(metric = "roc_auc")

show_best(dispneia_xgb_tune_grid, n = 6)

dispneia_best_step1 <-
  dispneia_xgb_tune_grid |>
  select_best(metric = "roc_auc")
```

```{r}
# best hiperparameters
glue::glue("The best number of trees was {dispneia_best_step1$trees} and learn_rate was {dispneia_best_step1$learn_rate}.")
```

### Step 2 -  Minimal node size (min_n) and depth tree (tree_depth)
### Model
```{r}
# Modelo
dispneia_xgb_model <-
  boost_tree(
  min_n = tune(),
  mtry = 0.8,
  trees = 25,
  tree_depth = tune(),
  learn_rate = 0.31,
  loss_reduction = 0,
  sample_size = 0.8
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_model) |> 
  add_recipe(dispneia_recipe)

```

### Tuning
```{r}
# Tune - min_n and tree_depth
dispneia_grid_xgb <-
  expand.grid(
    min_n = seq(1,16,2),
    tree_depth = seq(1,17,4)
)

dispneia_grid_xgb |> 
  DT::datatable()
```
```{r warning=FALSE}
doParallel::registerDoParallel(4)

dispneia_xgb_tune_grid <-
  tune_grid(
  dispneia_xgb_wf,
  resamples = dispneia_resamples,
  grid = dispneia_grid_xgb,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
  verbose = TRUE
  )
```

```{r warning=FALSE, out.width="100%"}
autoplot(dispneia_xgb_tune_grid) +
  theme_bw() +
  theme(legend.position = "top")

collect_metrics(dispneia_xgb_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  filter(mean == max(mean))

dispneia_xgb_tune_grid |> 
  select_best(metric = "roc_auc")


show_best(dispneia_xgb_tune_grid, n = 6)

dispneia_best_step2 <-
  dispneia_xgb_tune_grid |>
  select_best(metric = "roc_auc")
```

```{r warning=FALSE, out.width="100%"}
# best hiperparameters
glue::glue("The best minimal node size was {dispneia_best_step2$min_n} and depth tree was {dispneia_best_step2$tree_depth}.")
```
### Step 3 -  Number for the reduction in the loss function required to split further (loss_reduction)
### Model
```{r}
# Modelo
dispneia_xgb_model <-
  boost_tree(
  min_n = 11,
  mtry = 0.8,
  trees = 25,
  tree_depth = 1,
  learn_rate = 0.31,
  loss_reduction = tune(),
  sample_size = 0.8
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_model) |> 
  add_recipe(dispneia_recipe)

```

### Tuning
```{r}
# Tune - min_n and tree_depth
dispneia_grid_xgb <-
  expand.grid(
    loss_reduction = seq(1, 4, 0.04)
)

dispneia_grid_xgb |> 
  DT::datatable()
```
```{r warning=FALSE}
doParallel::registerDoParallel(4)

dispneia_xgb_tune_grid <-
  tune_grid(
  dispneia_xgb_wf,
  resamples = dispneia_resamples,
  grid = dispneia_grid_xgb,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
  verbose = TRUE
  )
```

```{r, out.width="100%"}
autoplot(dispneia_xgb_tune_grid) +
  theme_bw() 

collect_metrics(dispneia_xgb_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  filter(mean == max(mean))

dispneia_xgb_tune_grid |> 
  select_best(metric = "roc_auc")


show_best(dispneia_xgb_tune_grid, n = 6)

dispneia_best_step3 <-
  dispneia_xgb_tune_grid |>
  select_best(metric = "roc_auc")
```

```{r warning=FALSE, out.width="100%"}
# best hiperparameters
glue::glue("The best loss_reduction was {dispneia_best_step3$loss_reduction}.")
```

### Step 4 -  Number (or proportion) of predictors that will be randomly sampled (mtry) and number (or proportion) of data that is exposed to the fitting routine (sample_size)
### Model
```{r}
# Modelo
dispneia_xgb_model <-
  boost_tree(
  min_n = 11,
  mtry = tune(),
  trees = 25,
  tree_depth = 1,
  learn_rate = 0.31,
  loss_reduction = 1.36,
  sample_size = tune()
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_model) |> 
  add_recipe(dispneia_recipe)

```

### Tuning
```{r}
# Tune - min_n and tree_depth
dispneia_grid_xgb <-
  expand.grid(
    mtry = seq(0.2, 1.0, 0.2),
    sample_size = seq(0.2, 1.0,by = 0.2)
)

dispneia_grid_xgb |> 
  DT::datatable()
```

```{r warning=FALSE}
doParallel::registerDoParallel(4)

dispneia_xgb_tune_grid <-
  tune_grid(
  dispneia_xgb_wf,
  resamples = dispneia_resamples,
  grid = dispneia_grid_xgb,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
  verbose = TRUE
  )
```

```{r, out.width="100%"}
autoplot(dispneia_xgb_tune_grid) +
  theme_bw() +
  theme(legend.position = "top")

collect_metrics(dispneia_xgb_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  filter(mean == max(mean))

dispneia_xgb_tune_grid |> 
  select_best(metric = "roc_auc")

show_best(dispneia_xgb_tune_grid, n = 6)

dispneia_best_step4 <-
  dispneia_xgb_tune_grid |>
  select_best(metric = "roc_auc")
```

```{r, out.width="100%"}
# best hiperparameters
glue::glue("The best number (or proportion) of predictors that will be randomly sampled (mtry) was {dispneia_best_step4$mtry} and the number (or proportion) of data that is exposed to the fitting routine (sample size) was {dispneia_best_step4$sample_size}.")
```

### Step 5 -  Training again learing rate and trees with lower values
### Model
```{r}
# Modelo
dispneia_xgb_model <-
  boost_tree(
  min_n = 11,
  mtry = 0.8,
  trees = tune(),
  tree_depth = 1,
  learn_rate = tune(),
  loss_reduction = 1.36,
  sample_size = 0.8
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_model) |> 
  add_recipe(dispneia_recipe)
```

### Tuning
```{r}
# Tune - min_n and tree_depth
dispneia_grid_xgb <-
  expand.grid(
    trees = seq(15, 700, 25),
    learn_rate = seq(0.02, 0.1, 0.02)
)

dispneia_grid_xgb |> 
  DT::datatable()
```

```{r warning=FALSE}
doParallel::registerDoParallel(4)

dispneia_xgb_tune_grid <-
  tune_grid(
  dispneia_xgb_wf,
  resamples = dispneia_resamples,
  grid = dispneia_grid_xgb,
  metrics = metric_set(roc_auc, accuracy, sensitivity, specificity),
  verbose = TRUE
  )
```

```{r, out.width="100%"}
autoplot(dispneia_xgb_tune_grid) +
  theme_bw() +
  theme(legend.position = "top")

collect_metrics(dispneia_xgb_tune_grid) |> 
  filter(.metric == "roc_auc") |> 
  filter(mean == max(mean))

dispneia_xgb_tune_grid |> 
  select_best(metric = "roc_auc")

show_best(dispneia_xgb_tune_grid, n = 6)

dispneia_best_step5 <-
  dispneia_xgb_tune_grid |>
  select_best(metric = "roc_auc")
```

```{r}
# best hiperparameters
glue::glue("Following tunning again, the best number of trees was {dispneia_best_step5$trees} and learn_rate was {dispneia_best_step5$learn_rate}.")
```

### Final model performance
### Model
```{r}
# Model
dispneia_xgb_final_model <-
  boost_tree(
  min_n = 11,
  mtry = 0.8,
  trees = 565,
  tree_depth = 1,
  learn_rate = 0.02,
  loss_reduction = 1.36,
  sample_size = 0.8
) |>
  set_mode("classification") |>
  set_engine("xgboost", count = FALSE)
```

### Workflow
```{r}
# Workflow
dispneia_xgb_final_wf <- 
  workflow() |> 
  add_model(dispneia_xgb_final_model) |> 
  add_recipe(dispneia_recipe)
```

### Tuning
```{r}
# Tune - min_n and tree_depth
dispneia_best_params_xgb <-
  expand.grid(
    min_n = 11,
    mtry = 0.8,
    trees = 565,
    tree_depth = 1,
    learn_rate = 0.02,
    loss_reduction = 1.36,
    sample_size = 0.8
)

dispneia_best_params_xgb |> 
  DT::datatable()
```

```{r warning=FALSE}
# Desempenho do modelo finaL ----------------------------------------------
dispneia_xgb_final_wf <- 
  dispneia_xgb_final_wf |>
  finalize_workflow(dispneia_best_params_xgb)

dispneia_xgb_last_fit <-
  last_fit(dispneia_xgb_final_wf, dispneia_initial_split)

collect_predictions(dispneia_xgb_last_fit) |> 
  mutate(modelo = "XGBoost") |> 
  group_by(modelo) |> 
  roc_curve(dispneia, .pred_No) |> 
  autoplot() +
  theme_bw() +
  theme(legend.position = "none")

```

```{r warning=FALSE}
# best hiperparameters
best_roc <- show_best(dispneia_xgb_last_fit)
glue::glue("The best ROC was {round(best_roc$mean,2)}!")
```


```{r}
dispneia_xgb_last_fit_model <- dispneia_xgb_last_fit$.workflow[[1]]$fit$fit

dispneia_xgb_vip <- 
  extract_fit_engine(dispneia_xgb_last_fit)

vip::vip(dispneia_xgb_vip)+
  theme_bw()

```

# Conclusions
Finally, XGBoost was effective to predict dyspnea in COVID-19 survivors 6-9 months after hospital discharge. Thus, XGBoost implementation in a clinical setting might aid health professionals in the early management of COVID-19 survivors at risk of future dyspnea.

It is noteworthy  that the analyses performed herein are not intended to be used in practice. However, these analysis reinforce the utilization of ML algorithm in the health care.

Hope you enjoyed it!`r emojifont::emoji("stuck_out_tongue_winking_eye")`!

![](https://us.123rf.com/450wm/alesika/alesika2008/alesika200800157/153702004-see-you-soon-inscription-handwritten-lettering-illustration-black-vector-text-in-speech-bubble-simpl.jpg?ver=6)


# References
[Srazzo et al., Long-term dyspnea, regional ventilation distribution and peripheral lung function in COVID-19 survivors: a 1 year follow-up study](https://bmcpulmmed.biomedcentral.com/articles/10.1186/s12890-022-02214-5)

[An Introduction to Statistical Learning: with Applications in R](https://www.statlearning.com/)
