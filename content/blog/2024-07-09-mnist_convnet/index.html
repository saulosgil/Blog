---
title: "Deep Learning - A simple CNN to predict Handwritten Digits - MNIST dataset"
author: "Saulo Gil"
date: "2024-07-10"
excerpt: ''
subtitle: ''
draft: false
images: null
series: null
layout: single
slug: []
categories: []
tags: []
---



<!-- badges: start -->
<!-- badges: end -->
<div id="what-is-mnist-dataset" class="section level2">
<h2>ğŸ“’ What is MNIST datasetâ“</h2>
<p>Introduced by Yann LeCun and colleagues in the 1990s, the MNIST dataset has played a significant role in the development and evaluation of new algorithms in the field of machine learning and deep learning.</p>
<p>The MNIST dataset (Modified National Institute of Standards and Technology dataset) consists of a large database of handwritten digits. Here are some key points about the MNIST dataset:</p>
<ol style="list-style-type: decimal">
<li><strong>Content</strong>: It contains 60,000 training images and 10,000 testing images of handwritten digits (0-9). Each image is 28x28 pixels in grayscale.</li>
<li><strong>Format</strong>: The images are standardized, meaning each image is centered and size-normalized. Each pixel value ranges from 0 to 255, where 0 represents white and 255 represents black.</li>
<li><strong>Usage</strong>: The dataset is widely used for benchmarking machine learning algorithms, particularly in the field of image recognition and classification. Itâ€™s a common introductory dataset for learning and experimenting with neural networks and other machine learning models.</li>
<li><strong>Accessibility</strong>: The dataset is publicly available and can be easily accessed through various machine learning libraries, such as TensorFlow, PyTorch, and Keras.</li>
</ol>
</div>
<div id="what-is-convolutional-neural-network---cnn" class="section level2">
<h2>ğŸ“’ What is Convolutional Neural Network - CNNâ“</h2>
<p>A CNN is a type of deep learning algorithm specifically designed for processing structured grid data, such as images. CNNs are particularly effective for tasks such as image recognition, classification, and detection due to their ability to automatically and adaptively learn spatial hierarchies of features.</p>
<p>Here are some key components and concepts associated with CNNs:</p>
<ol style="list-style-type: decimal">
<li><strong>Convolutional Layers</strong>: These layers apply a convolution operation to the input, passing the result to the next layer. Convolutional layers consist of a set of learnable filters (or kernels) that are convolved with the input image to produce feature maps. Each filter detects different features, such as edges, textures, or more complex patterns.</li>
<li><strong>ReLU Activation Function</strong>: After each convolution operation, an activation function like the Rectified Linear Unit (ReLU) is applied. ReLU introduces non-linearity to the model, enabling it to learn more complex patterns.</li>
<li><strong>Pooling Layers</strong>: Also known as subsampling or downsampling, pooling layers reduce the dimensionality of each feature map while retaining the most important information. Max pooling, which takes the maximum value from a set of values in a feature map, is a common type of pooling.</li>
<li><strong>Fully Connected Layers</strong>: These layers are typically used towards the end of the network. Neurons in a fully connected layer have connections to all activations in the previous layer. They are responsible for combining the features extracted by the convolutional and pooling layers to make the final classification or prediction.</li>
<li><strong>Flattening</strong>: Before passing the data to fully connected layers, the output from the convolutional and pooling layers is often flattened into a one-dimensional vector.</li>
<li><strong>Dropout</strong>: This regularization technique involves randomly setting a fraction of input units to zero during training. Dropout helps prevent overfitting by ensuring that the model generalizes better to new data.</li>
<li><strong>Training</strong>: CNNs are trained using backpropagation and gradient descent. The objective is to minimize the loss function by adjusting the weights and biases of the network through iterative updates.</li>
</ol>
<p>CNNs have achieved state-of-the-art performance in many computer vision tasks, including object detection, facial recognition, and medical image analysis, among others.</p>
<p>Therefore, I utilized a CNN to predict handwritten digits from the MNIST dataset.</p>
<div id="lets-do-it" class="section level3">
<h3>Letâ€™s do itâ—</h3>
</div>
</div>
<div id="programming-language" class="section level2">
<h2>ğŸ‘¨â€ğŸ’» Programming language</h2>
<ul>
<li><img src="https://img.shields.io/badge/Python-gray?style=flat&amp;logo=python&amp;logoColor=white" /></li>
</ul>
</div>
<div id="libraries-necessaries" class="section level2">
<h2>ğŸ“¦ Libraries necessaries</h2>
<pre class="python"><code>import numpy as np
import keras
from keras import layers
import matplotlib.pyplot as plt
import tensorflow as tf</code></pre>
</div>
<div id="preparing-the-data" class="section level1">
<h1>ğŸ’» Preparing the data</h1>
<pre class="python"><code># Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# Load the data and split it between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the 0 and 1
x_train = x_train.astype(&quot;float32&quot;) / 255
x_test = x_test.astype(&quot;float32&quot;) / 255

# Make sure images have shape (28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print(&quot;x_train shape:&quot;, x_train.shape)</code></pre>
<pre><code>## x_train shape: (60000, 28, 28, 1)</code></pre>
<pre class="python"><code>print(x_train.shape[0], &quot;train samples&quot;)</code></pre>
<pre><code>## 60000 train samples</code></pre>
<pre class="python"><code>print(x_test.shape[0], &quot;test samples&quot;)</code></pre>
<pre><code>## 10000 test samples</code></pre>
<pre class="python"><code>
# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)</code></pre>
<div id="looking-handwritten-digits-images" class="section level2">
<h2>ğŸ§ Looking handwritten digits images</h2>
<pre class="python"><code>plt.figure()
plt.imshow(x_train[0], cmap=plt.cm.binary)
plt.show()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Leâ€™ts see 10 imagens and its labels.</p>
<pre class="python"><code># Ten images and labels
plt.figure(figsize=(15,10))
for i in range(25):
  plt.subplot(5,5,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.imshow(x_train[i], cmap=plt.cm.binary)
  plt.xlabel(f&#39;Label: {np.argmax(y_train[i])}&#39;)
plt.show()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-3.png" width="1440" /></p>
</div>
<div id="model---convolutional-neural-network---cnn" class="section level2">
<h2>ğŸ’»ğŸ§  Model - Convolutional Neural Network - CNN</h2>
<p>The CNN architecture used was developmented by FranÃ§ois Chollet.</p>
<p>This CNN achieved ~99% test accuracy.</p>
<p>The original code is available in the <a href="https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/mnist_convnet.ipynb#scrollTo=xo9HwpE5gmQB">Google Colab</a>.</p>
<div id="who-is-franÃ§ois-chollet" class="section level3">
<h3>ğŸ¤” Who is FranÃ§ois Chollet?</h3>
<p>FranÃ§ois Chollet is a well-known figure in the field of artificial intelligence and deep learning.</p>
<p>He is the creator of Keras, one of the most popular libraries for deep learning (being used hereâ—), an interface for the TensorFlow library, among others, that simplifies the process of developing complex neural networks.</p>
<p>He has contributed to various research areas within machine learning and artificial intelligence, including computer vision, natural language processing, and the development of new machine learning architectures.</p>
<p>So, it is a good idea to use a CNN architecture development by him ğŸ˜œ.</p>
<div id="model-summary" class="section level4">
<h4>Model Summary</h4>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ<span style="font-weight: bold"> Layer (type)                    </span>â”ƒ<span style="font-weight: bold"> Output Shape           </span>â”ƒ<span style="font-weight: bold">       Param # </span>â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ conv2d (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)                 â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">26</span>, <span style="color: #00af00; text-decoration-color: #00af00">26</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)     â”‚           <span style="color: #00af00; text-decoration-color: #00af00">320</span> â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)    â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>, <span style="color: #00af00; text-decoration-color: #00af00">13</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)     â”‚             <span style="color: #00af00; text-decoration-color: #00af00">0</span> â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv2d_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Conv2D</span>)               â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">11</span>, <span style="color: #00af00; text-decoration-color: #00af00">11</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)     â”‚        <span style="color: #00af00; text-decoration-color: #00af00">18,496</span> â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_pooling2d_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">MaxPooling2D</span>)  â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">5</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)       â”‚             <span style="color: #00af00; text-decoration-color: #00af00">0</span> â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten (<span style="color: #0087ff; text-decoration-color: #0087ff">Flatten</span>)               â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1600</span>)           â”‚             <span style="color: #00af00; text-decoration-color: #00af00">0</span> â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)               â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1600</span>)           â”‚             <span style="color: #00af00; text-decoration-color: #00af00">0</span> â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                   â”‚ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">10</span>)             â”‚        <span style="color: #00af00; text-decoration-color: #00af00">16,010</span> â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
<ul>
<li><p>Total params: 34,826 (136.04 KB)</p></li>
<li><p>Trainable params: 34,826 (136.04 KB)</p></li>
<li><p>Non-trainable params: 0 (0.00 B)</p></li>
</ul>
</div>
</div>
</div>
<div id="defining-a-callback-loss-function-optimizer-and-a-model-performance-metric." class="section level2">
<h2>ğŸ‘¨â€ğŸ’» Defining a callback, loss function, optimizer and, a model performance metric.</h2>
<pre class="python"><code>batch_size = 128
epochs = 15

# Callback to interrupt the training when accuracy achieves 99% (implemented just for testing!)
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get(&#39;accuracy&#39;)&gt;0.99):
      print(&quot;\nReached 99% accuracy so cancelling training!&quot;)
      self.model.stop_training = True

callbacks = myCallback()


# Model compile with loss function, optimizer e metrics
model.compile(loss=&#39;categorical_crossentropy&#39;,
              optimizer=&#39;adam&#39;,
              metrics=[&#39;accuracy&#39;])</code></pre>
</div>
<div id="training-the-model" class="section level2">
<h2>ğŸ–¥ï¸ğŸª« Training the model</h2>
<pre class="python"><code>model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[callbacks], validation_split=0.1, verbose=2)</code></pre>
<pre><code>## Epoch 1/15
## 422/422 - 6s - 14ms/step - accuracy: 0.8809 - loss: 0.3828 - val_accuracy: 0.9770 - val_loss: 0.0876
## Epoch 2/15
## 422/422 - 5s - 11ms/step - accuracy: 0.9644 - loss: 0.1178 - val_accuracy: 0.9802 - val_loss: 0.0678
## Epoch 3/15
## 422/422 - 5s - 12ms/step - accuracy: 0.9730 - loss: 0.0892 - val_accuracy: 0.9857 - val_loss: 0.0520
## Epoch 4/15
## 422/422 - 5s - 13ms/step - accuracy: 0.9770 - loss: 0.0737 - val_accuracy: 0.9868 - val_loss: 0.0460
## Epoch 5/15
## 422/422 - 6s - 14ms/step - accuracy: 0.9799 - loss: 0.0644 - val_accuracy: 0.9897 - val_loss: 0.0383
## Epoch 6/15
## 422/422 - 6s - 14ms/step - accuracy: 0.9819 - loss: 0.0585 - val_accuracy: 0.9910 - val_loss: 0.0371
## Epoch 7/15
## 422/422 - 5s - 13ms/step - accuracy: 0.9834 - loss: 0.0527 - val_accuracy: 0.9910 - val_loss: 0.0369
## Epoch 8/15
## 422/422 - 5s - 12ms/step - accuracy: 0.9849 - loss: 0.0480 - val_accuracy: 0.9908 - val_loss: 0.0350
## Epoch 9/15
## 422/422 - 6s - 15ms/step - accuracy: 0.9857 - loss: 0.0454 - val_accuracy: 0.9902 - val_loss: 0.0332
## Epoch 10/15
## 422/422 - 7s - 16ms/step - accuracy: 0.9869 - loss: 0.0420 - val_accuracy: 0.9913 - val_loss: 0.0311
## Epoch 11/15
## 422/422 - 6s - 14ms/step - accuracy: 0.9868 - loss: 0.0413 - val_accuracy: 0.9913 - val_loss: 0.0318
## Epoch 12/15
## 422/422 - 6s - 13ms/step - accuracy: 0.9878 - loss: 0.0374 - val_accuracy: 0.9912 - val_loss: 0.0316
## Epoch 13/15
## 422/422 - 5s - 12ms/step - accuracy: 0.9889 - loss: 0.0371 - val_accuracy: 0.9915 - val_loss: 0.0296
## Epoch 14/15
## 422/422 - 5s - 13ms/step - accuracy: 0.9889 - loss: 0.0350 - val_accuracy: 0.9918 - val_loss: 0.0326
## Epoch 15/15
## 422/422 - 6s - 14ms/step - accuracy: 0.9892 - loss: 0.0342 - val_accuracy: 0.9918 - val_loss: 0.0293
## &lt;keras.src.callbacks.history.History object at 0x0000023D34423860&gt;</code></pre>
</div>
<div id="evaluate-the-trained-model" class="section level2">
<h2>ğŸ¯ Evaluate the trained model</h2>
<pre class="python"><code>score = model.evaluate(x_test, y_test, verbose=0)
print(&quot;Test loss:&quot;, round(score[0], 2))</code></pre>
<pre><code>## Test loss: 0.03</code></pre>
<pre class="python"><code>print(&quot;Test accuracy:&quot;, round(score[1], 2))</code></pre>
<pre><code>## Test accuracy: 0.99</code></pre>
</div>
<div id="looking-the-predictions" class="section level2">
<h2>ğŸ§ Looking the predictions</h2>
<pre class="python"><code>predictions = model.predict(x_test)</code></pre>
<pre><code>## 
## [1m  1/313[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m15s[0m 49ms/step
## [1m 21/313[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step  
## [1m 39/313[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step
## [1m 75/313[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 2ms/step
## [1m116/313[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 2ms/step
## [1m152/313[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 2ms/step
## [1m183/313[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 2ms/step
## [1m212/313[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 2ms/step
## [1m248/313[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 2ms/step
## [1m285/313[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step
## [1m313/313[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 2ms/step
## [1m313/313[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 2ms/step</code></pre>
<div id="example---index-2" class="section level3">
<h3>Example - index 2</h3>
<pre class="python"><code>np.array(predictions[2])</code></pre>
<pre><code>## array([2.1943085e-06, 9.9925309e-01, 1.6262649e-05, 3.9057147e-07,
##        5.0057319e-04, 6.9738951e-07, 8.6912733e-06, 1.2572407e-04,
##        9.1377900e-05, 9.2568655e-07], dtype=float32)</code></pre>
<pre class="python"><code>np.argmax(predictions[2])</code></pre>
<pre><code>## 1</code></pre>
<p>As we can see, the highest probability is at index 1 (i.e., 9.99), indicating the value 1.</p>
</div>
</div>
<div id="lets-see-the-image" class="section level2">
<h2>Letâ€™s see the imageâ—</h2>
<pre class="python"><code>plt.figure()
plt.imshow(x_test[2], cmap=plt.cm.binary)
plt.show()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-5.png" width="672" /></p>
</div>
<div id="now-lets-see-25-examples-and-predicted-labels." class="section level2">
<h2>ğŸ§ Now, letâ€™s see 25 examples and predicted labels.</h2>
<pre class="python"><code>plt.figure(figsize=(15,10))
for i in range(25):
  plt.subplot(5,5,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.imshow(x_test[i], cmap=plt.cm.binary)
  plt.xlabel(f&#39;Predicted label: {np.argmax(predictions[i])}&#39;)
plt.show()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-7.png" width="1440" /></p>
</div>
<div id="conclusion" class="section level2">
<h2>ğŸ¤“ Conclusion</h2>
<p>Indeed, the CNN architecture was able to predict accurately handwritten digits.</p>
<p>This is a simple application of CNNs, but it is noteworthy that this technique has been used to address complex real-world problems.</p>
<p>Hope you enjoyed it!</p>
<p><img src="https://us.123rf.com/450wm/alesika/alesika2008/alesika200800157/153702004-see-you-soon-inscription-handwritten-lettering-illustration-black-vector-text-in-speech-bubble-simpl.jpg?ver=6" /></p>
</div>
<div id="reference" class="section level2">
<h2>Reference</h2>
<ol style="list-style-type: decimal">
<li><a href="http://yann.lecun.com/exdb/mnist/">The MNIST database of handwritten digits</a></li>
<li><a href="https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/mnist_convnet.ipynb#scrollTo=xo9HwpE5gmQB">Chollet, F. A simple convnet that achieves ~99% test accuracy on MNIST.</a></li>
<li><a href="https://github.com/saulosgil/mnist_convnet">My Jupyter Notebook with Complete Code</a></li>
</ol>
</div>
</div>
